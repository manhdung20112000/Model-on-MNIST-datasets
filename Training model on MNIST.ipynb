{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* We should define transform function before load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, ), (0.5, )),\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%c:\\users\\manhd\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\datasets\\mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_set = datasets.MNIST(root=DATA_PATH, download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST(root=DATA_PATH, download=True, train=False, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [15, 8]\n",
    "fig, axs = plt.subplots(2, 4)\n",
    "\n",
    "index = 0\n",
    "for i in range(2):\n",
    "    for j in range(4):\n",
    "        axs[i][j].imshow(images[index].numpy().squeeze(), cmap=\"gray\")\n",
    "        axs[i][j].axis('off')\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.net = nn.Sequential(OrderedDict([\n",
    "                ('conv1', nn.Conv2d(in_channels=1, out_channels=20, kernel_size=4, padding=1)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('maxpool1', nn.MaxPool2d(kernel_size=2)),\n",
    "                ('conv2', nn.Conv2d(in_channels=20, out_channels=40, kernel_size=5, padding=1)),\n",
    "                ('relu2', nn.ReLU()),\n",
    "                ('maxpool2', nn.MaxPool2d(kernel_size=3)),\n",
    "                ('flatten', nn.Flatten()),\n",
    "                ('fc1', nn.Linear(in_features=40*3*3, out_features=150)),\n",
    "                ('relu3', nn.ReLU()),\n",
    "                ('fc2', nn.Linear(in_features=150, out_features=10)),\n",
    "            ]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        out = self.net(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 5\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Net()\n",
    "if (torch.cuda.is_available()):\n",
    "    network.cuda()\n",
    "\n",
    "optimizer = optim.SGD(network.parameters(), lr=1e-3, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        output = network(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append((batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "            \n",
    "            torch.save(network.state_dict(), './result/mnist_model.pth')\n",
    "            torch.save(optimizer.state_dict(), './result/mnist_optimizer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "            output = network(data)\n",
    "            test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    \n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-6c2218f37724>:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3110, Accuracy: 825/10000 (8%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.326431\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.320519\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.320432\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.329108\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.337043\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.318265\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.304482\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.316073\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.273014\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.275710\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.288960\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.296240\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.287566\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.302268\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.302526\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.313013\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.281964\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 2.280559\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 2.276327\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 2.237771\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.282819\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 2.302974\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 2.247258\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 2.280625\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 2.259986\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.240849\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 2.257557\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 2.238181\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 2.223041\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 2.256007\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.251918\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 2.207698\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 2.259933\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 2.219938\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 2.236853\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.245749\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 2.178530\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 2.124925\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 2.156496\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 2.107459\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.161909\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 2.110641\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 2.125816\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 2.096934\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 2.030041\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.070225\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 1.993088\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 2.014853\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 1.953857\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 2.011914\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.986192\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 1.892086\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 1.892547\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 1.831731\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 1.741452\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 1.788529\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 1.638576\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 1.548198\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 1.461428\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 1.701717\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.616470\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 1.630531\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 1.620855\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 1.477747\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 1.499384\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 1.533371\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 1.373865\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 1.518168\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 1.395115\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 1.201671\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.233149\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 1.288772\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 1.028959\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 1.206240\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 1.140789\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 1.081608\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 1.163632\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 1.107100\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 1.187290\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.995526\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.081380\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.981296\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 1.028309\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 1.064138\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 1.120632\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.029377\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 1.123272\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 1.062560\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.878378\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 1.037803\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.122998\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 1.022938\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.800697\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.859285\n",
      "\n",
      "Test set: Avg. loss: 0.5083, Accuracy: 8742/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.945089\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.830393\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.826626\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.920488\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.763003\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.882631\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 1.063745\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.867258\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.562710\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.750866\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.720355\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 1.023167\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 1.041339\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.863783\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.630151\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.546023\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.815895\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.616840\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.732358\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.654304\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.979923\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.776011\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.831648\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.885636\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.809828\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.639110\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 1.035708\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.677877\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.597585\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.650251\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.736456\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.643240\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.746431\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.552200\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.773712\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.418720\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.627170\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.738953\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.836834\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.526874\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.781402\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.580637\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.471472\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.594964\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.669636\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.788756\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.693573\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.645325\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.797562\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.640574\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.611448\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.626932\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.600622\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.628722\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.622926\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.586532\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.721470\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.712155\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.556142\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.544672\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.961991\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.470746\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.633862\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.813174\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.549476\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.633226\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.446791\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.531081\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.526243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.778889\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.495644\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.546931\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.694092\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.480372\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.505666\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.648416\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.600455\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.500947\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.535179\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.739102\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.698588\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.579799\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.284415\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.452977\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.495553\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.833893\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.587030\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.610820\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.558814\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.556372\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.441162\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.635346\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.483057\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.438890\n",
      "\n",
      "Test set: Avg. loss: 0.2391, Accuracy: 9321/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.349266\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.529460\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.413385\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.729680\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.414188\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.365896\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.710702\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.795471\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.594334\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.367062\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.593797\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.793376\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.469759\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.554064\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.743367\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.462565\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.490676\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.752637\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.577276\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.594121\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.691808\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.261803\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.399882\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.535887\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.585303\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.559214\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.396293\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.392483\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.363713\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.469657\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.472685\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.476959\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.422446\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.509128\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.500124\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.325767\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.380354\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.573715\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.474939\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.738970\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.353757\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.552844\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.306006\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.517908\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.346022\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.329205\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.498500\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.451860\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.582949\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.501740\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.398831\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.465563\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.489660\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.301789\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.541687\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.501731\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.436860\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.470106\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.350129\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.279722\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.499563\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.459110\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.489933\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.361769\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.368016\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.464323\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.437309\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.404739\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.409608\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.362360\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.314478\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.435179\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.362565\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.393375\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.507253\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.351066\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.539565\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.472825\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.415669\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.368443\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.340726\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.551482\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.303080\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.456933\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.647146\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.387026\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.544127\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.358986\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.324166\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.283562\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.364156\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.499205\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.308385\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.645704\n",
      "\n",
      "Test set: Avg. loss: 0.1730, Accuracy: 9472/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.347828\n",
      "Train Epoch: 4 [640/60000 (1%)]\tLoss: 0.926686\n",
      "Train Epoch: 4 [1280/60000 (2%)]\tLoss: 0.378798\n",
      "Train Epoch: 4 [1920/60000 (3%)]\tLoss: 0.357760\n",
      "Train Epoch: 4 [2560/60000 (4%)]\tLoss: 0.522636\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.430385\n",
      "Train Epoch: 4 [3840/60000 (6%)]\tLoss: 0.644627\n",
      "Train Epoch: 4 [4480/60000 (7%)]\tLoss: 0.366095\n",
      "Train Epoch: 4 [5120/60000 (9%)]\tLoss: 0.375203\n",
      "Train Epoch: 4 [5760/60000 (10%)]\tLoss: 0.577122\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.242102\n",
      "Train Epoch: 4 [7040/60000 (12%)]\tLoss: 0.419074\n",
      "Train Epoch: 4 [7680/60000 (13%)]\tLoss: 0.303254\n",
      "Train Epoch: 4 [8320/60000 (14%)]\tLoss: 0.422625\n",
      "Train Epoch: 4 [8960/60000 (15%)]\tLoss: 0.490565\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.567199\n",
      "Train Epoch: 4 [10240/60000 (17%)]\tLoss: 0.457081\n",
      "Train Epoch: 4 [10880/60000 (18%)]\tLoss: 0.435679\n",
      "Train Epoch: 4 [11520/60000 (19%)]\tLoss: 0.312808\n",
      "Train Epoch: 4 [12160/60000 (20%)]\tLoss: 0.368185\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.430970\n",
      "Train Epoch: 4 [13440/60000 (22%)]\tLoss: 0.410692\n",
      "Train Epoch: 4 [14080/60000 (23%)]\tLoss: 0.330112\n",
      "Train Epoch: 4 [14720/60000 (25%)]\tLoss: 0.528856\n",
      "Train Epoch: 4 [15360/60000 (26%)]\tLoss: 0.275230\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.242029\n",
      "Train Epoch: 4 [16640/60000 (28%)]\tLoss: 0.413871\n",
      "Train Epoch: 4 [17280/60000 (29%)]\tLoss: 0.396970\n",
      "Train Epoch: 4 [17920/60000 (30%)]\tLoss: 0.470272\n",
      "Train Epoch: 4 [18560/60000 (31%)]\tLoss: 0.285369\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.435855\n",
      "Train Epoch: 4 [19840/60000 (33%)]\tLoss: 0.453917\n",
      "Train Epoch: 4 [20480/60000 (34%)]\tLoss: 0.299518\n",
      "Train Epoch: 4 [21120/60000 (35%)]\tLoss: 0.449446\n",
      "Train Epoch: 4 [21760/60000 (36%)]\tLoss: 0.448356\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.374587\n",
      "Train Epoch: 4 [23040/60000 (38%)]\tLoss: 0.291819\n",
      "Train Epoch: 4 [23680/60000 (39%)]\tLoss: 0.406144\n",
      "Train Epoch: 4 [24320/60000 (41%)]\tLoss: 0.360254\n",
      "Train Epoch: 4 [24960/60000 (42%)]\tLoss: 0.569167\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.371336\n",
      "Train Epoch: 4 [26240/60000 (44%)]\tLoss: 0.300114\n",
      "Train Epoch: 4 [26880/60000 (45%)]\tLoss: 0.354405\n",
      "Train Epoch: 4 [27520/60000 (46%)]\tLoss: 0.403908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [28160/60000 (47%)]\tLoss: 0.350996\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.670066\n",
      "Train Epoch: 4 [29440/60000 (49%)]\tLoss: 0.533302\n",
      "Train Epoch: 4 [30080/60000 (50%)]\tLoss: 0.338328\n",
      "Train Epoch: 4 [30720/60000 (51%)]\tLoss: 0.425237\n",
      "Train Epoch: 4 [31360/60000 (52%)]\tLoss: 0.266435\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.224197\n",
      "Train Epoch: 4 [32640/60000 (54%)]\tLoss: 0.279921\n",
      "Train Epoch: 4 [33280/60000 (55%)]\tLoss: 0.183884\n",
      "Train Epoch: 4 [33920/60000 (57%)]\tLoss: 0.395121\n",
      "Train Epoch: 4 [34560/60000 (58%)]\tLoss: 0.382958\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.238159\n",
      "Train Epoch: 4 [35840/60000 (60%)]\tLoss: 0.322465\n",
      "Train Epoch: 4 [36480/60000 (61%)]\tLoss: 0.304428\n",
      "Train Epoch: 4 [37120/60000 (62%)]\tLoss: 0.379463\n",
      "Train Epoch: 4 [37760/60000 (63%)]\tLoss: 0.410000\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.444524\n",
      "Train Epoch: 4 [39040/60000 (65%)]\tLoss: 0.200833\n",
      "Train Epoch: 4 [39680/60000 (66%)]\tLoss: 0.462550\n",
      "Train Epoch: 4 [40320/60000 (67%)]\tLoss: 0.427328\n",
      "Train Epoch: 4 [40960/60000 (68%)]\tLoss: 0.318470\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.483130\n",
      "Train Epoch: 4 [42240/60000 (70%)]\tLoss: 0.316293\n",
      "Train Epoch: 4 [42880/60000 (71%)]\tLoss: 0.678594\n",
      "Train Epoch: 4 [43520/60000 (72%)]\tLoss: 0.235858\n",
      "Train Epoch: 4 [44160/60000 (74%)]\tLoss: 0.558967\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.310168\n",
      "Train Epoch: 4 [45440/60000 (76%)]\tLoss: 0.329001\n",
      "Train Epoch: 4 [46080/60000 (77%)]\tLoss: 0.535365\n",
      "Train Epoch: 4 [46720/60000 (78%)]\tLoss: 0.238812\n",
      "Train Epoch: 4 [47360/60000 (79%)]\tLoss: 0.355605\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.304248\n",
      "Train Epoch: 4 [48640/60000 (81%)]\tLoss: 0.280376\n",
      "Train Epoch: 4 [49280/60000 (82%)]\tLoss: 0.281336\n",
      "Train Epoch: 4 [49920/60000 (83%)]\tLoss: 0.386185\n",
      "Train Epoch: 4 [50560/60000 (84%)]\tLoss: 0.354658\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.282745\n",
      "Train Epoch: 4 [51840/60000 (86%)]\tLoss: 0.229291\n",
      "Train Epoch: 4 [52480/60000 (87%)]\tLoss: 0.355444\n",
      "Train Epoch: 4 [53120/60000 (88%)]\tLoss: 0.394485\n",
      "Train Epoch: 4 [53760/60000 (90%)]\tLoss: 0.228985\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.201930\n",
      "Train Epoch: 4 [55040/60000 (92%)]\tLoss: 0.357877\n",
      "Train Epoch: 4 [55680/60000 (93%)]\tLoss: 0.236803\n",
      "Train Epoch: 4 [56320/60000 (94%)]\tLoss: 0.180891\n",
      "Train Epoch: 4 [56960/60000 (95%)]\tLoss: 0.372495\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.620670\n",
      "Train Epoch: 4 [58240/60000 (97%)]\tLoss: 0.299106\n",
      "Train Epoch: 4 [58880/60000 (98%)]\tLoss: 0.535813\n",
      "Train Epoch: 4 [59520/60000 (99%)]\tLoss: 0.669869\n",
      "\n",
      "Test set: Avg. loss: 0.1382, Accuracy: 9564/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.398005\n",
      "Train Epoch: 5 [640/60000 (1%)]\tLoss: 0.312031\n",
      "Train Epoch: 5 [1280/60000 (2%)]\tLoss: 0.241430\n",
      "Train Epoch: 5 [1920/60000 (3%)]\tLoss: 0.380365\n",
      "Train Epoch: 5 [2560/60000 (4%)]\tLoss: 0.491219\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.381603\n",
      "Train Epoch: 5 [3840/60000 (6%)]\tLoss: 0.532148\n",
      "Train Epoch: 5 [4480/60000 (7%)]\tLoss: 0.396358\n",
      "Train Epoch: 5 [5120/60000 (9%)]\tLoss: 0.352175\n",
      "Train Epoch: 5 [5760/60000 (10%)]\tLoss: 0.232005\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.293621\n",
      "Train Epoch: 5 [7040/60000 (12%)]\tLoss: 0.453830\n",
      "Train Epoch: 5 [7680/60000 (13%)]\tLoss: 0.252056\n",
      "Train Epoch: 5 [8320/60000 (14%)]\tLoss: 0.301722\n",
      "Train Epoch: 5 [8960/60000 (15%)]\tLoss: 0.504363\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.246415\n",
      "Train Epoch: 5 [10240/60000 (17%)]\tLoss: 0.430895\n",
      "Train Epoch: 5 [10880/60000 (18%)]\tLoss: 0.285454\n",
      "Train Epoch: 5 [11520/60000 (19%)]\tLoss: 0.634339\n",
      "Train Epoch: 5 [12160/60000 (20%)]\tLoss: 0.327244\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.294116\n",
      "Train Epoch: 5 [13440/60000 (22%)]\tLoss: 0.371915\n",
      "Train Epoch: 5 [14080/60000 (23%)]\tLoss: 0.418279\n",
      "Train Epoch: 5 [14720/60000 (25%)]\tLoss: 0.614591\n",
      "Train Epoch: 5 [15360/60000 (26%)]\tLoss: 0.215031\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.362567\n",
      "Train Epoch: 5 [16640/60000 (28%)]\tLoss: 0.342006\n",
      "Train Epoch: 5 [17280/60000 (29%)]\tLoss: 0.316636\n",
      "Train Epoch: 5 [17920/60000 (30%)]\tLoss: 0.279348\n",
      "Train Epoch: 5 [18560/60000 (31%)]\tLoss: 0.225032\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.326028\n",
      "Train Epoch: 5 [19840/60000 (33%)]\tLoss: 0.480354\n",
      "Train Epoch: 5 [20480/60000 (34%)]\tLoss: 0.306649\n",
      "Train Epoch: 5 [21120/60000 (35%)]\tLoss: 0.450119\n",
      "Train Epoch: 5 [21760/60000 (36%)]\tLoss: 0.358642\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.184913\n",
      "Train Epoch: 5 [23040/60000 (38%)]\tLoss: 0.354774\n",
      "Train Epoch: 5 [23680/60000 (39%)]\tLoss: 0.255261\n",
      "Train Epoch: 5 [24320/60000 (41%)]\tLoss: 0.255797\n",
      "Train Epoch: 5 [24960/60000 (42%)]\tLoss: 0.408691\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.226382\n",
      "Train Epoch: 5 [26240/60000 (44%)]\tLoss: 0.329384\n",
      "Train Epoch: 5 [26880/60000 (45%)]\tLoss: 0.191272\n",
      "Train Epoch: 5 [27520/60000 (46%)]\tLoss: 0.379686\n",
      "Train Epoch: 5 [28160/60000 (47%)]\tLoss: 0.205574\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.253937\n",
      "Train Epoch: 5 [29440/60000 (49%)]\tLoss: 0.386130\n",
      "Train Epoch: 5 [30080/60000 (50%)]\tLoss: 0.570693\n",
      "Train Epoch: 5 [30720/60000 (51%)]\tLoss: 0.283405\n",
      "Train Epoch: 5 [31360/60000 (52%)]\tLoss: 0.309291\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.243803\n",
      "Train Epoch: 5 [32640/60000 (54%)]\tLoss: 0.335724\n",
      "Train Epoch: 5 [33280/60000 (55%)]\tLoss: 0.196880\n",
      "Train Epoch: 5 [33920/60000 (57%)]\tLoss: 0.506390\n",
      "Train Epoch: 5 [34560/60000 (58%)]\tLoss: 0.165415\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.375286\n",
      "Train Epoch: 5 [35840/60000 (60%)]\tLoss: 0.280348\n",
      "Train Epoch: 5 [36480/60000 (61%)]\tLoss: 0.246329\n",
      "Train Epoch: 5 [37120/60000 (62%)]\tLoss: 0.372907\n",
      "Train Epoch: 5 [37760/60000 (63%)]\tLoss: 0.349086\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.198415\n",
      "Train Epoch: 5 [39040/60000 (65%)]\tLoss: 0.317080\n",
      "Train Epoch: 5 [39680/60000 (66%)]\tLoss: 0.391719\n",
      "Train Epoch: 5 [40320/60000 (67%)]\tLoss: 0.281507\n",
      "Train Epoch: 5 [40960/60000 (68%)]\tLoss: 0.279256\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.441303\n",
      "Train Epoch: 5 [42240/60000 (70%)]\tLoss: 0.345508\n",
      "Train Epoch: 5 [42880/60000 (71%)]\tLoss: 0.381895\n",
      "Train Epoch: 5 [43520/60000 (72%)]\tLoss: 0.343503\n",
      "Train Epoch: 5 [44160/60000 (74%)]\tLoss: 0.401365\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.330753\n",
      "Train Epoch: 5 [45440/60000 (76%)]\tLoss: 0.311037\n",
      "Train Epoch: 5 [46080/60000 (77%)]\tLoss: 0.323898\n",
      "Train Epoch: 5 [46720/60000 (78%)]\tLoss: 0.423473\n",
      "Train Epoch: 5 [47360/60000 (79%)]\tLoss: 0.364678\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.395296\n",
      "Train Epoch: 5 [48640/60000 (81%)]\tLoss: 0.366340\n",
      "Train Epoch: 5 [49280/60000 (82%)]\tLoss: 0.323427\n",
      "Train Epoch: 5 [49920/60000 (83%)]\tLoss: 0.451009\n",
      "Train Epoch: 5 [50560/60000 (84%)]\tLoss: 0.323950\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.389944\n",
      "Train Epoch: 5 [51840/60000 (86%)]\tLoss: 0.166601\n",
      "Train Epoch: 5 [52480/60000 (87%)]\tLoss: 0.207245\n",
      "Train Epoch: 5 [53120/60000 (88%)]\tLoss: 0.307618\n",
      "Train Epoch: 5 [53760/60000 (90%)]\tLoss: 0.296458\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.197969\n",
      "Train Epoch: 5 [55040/60000 (92%)]\tLoss: 0.262478\n",
      "Train Epoch: 5 [55680/60000 (93%)]\tLoss: 0.196971\n",
      "Train Epoch: 5 [56320/60000 (94%)]\tLoss: 0.250339\n",
      "Train Epoch: 5 [56960/60000 (95%)]\tLoss: 0.351186\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.448142\n",
      "Train Epoch: 5 [58240/60000 (97%)]\tLoss: 0.215157\n",
      "Train Epoch: 5 [58880/60000 (98%)]\tLoss: 0.180397\n",
      "Train Epoch: 5 [59520/60000 (99%)]\tLoss: 0.268193\n",
      "\n",
      "Test set: Avg. loss: 0.1162, Accuracy: 9631/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_interval = 10\n",
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
